# Hey you, when editing this file, please make duplicates of this file to make your own finetune configurations.

# Where the resulting models will be saved. Can be absolute or relative paths.
# Example: X:\output or /some/unix/path
checkpoint_path: output
# The name of the training run and model name
# Example: X:\output\unet\stage_c_1b_finetuning.safetensors or /some/unix/path/output/unet/stage_c_1b_finetuning.safetensors
# This will also name the output checkpoints.
experiment_id: stage_c_1b_finetuning

# Stage C parameters:
# Valid settings: 1B or 3.6B
model_version: 1B

# The checkpoints that are used for training.
effnet_checkpoint_path: models/effnet_encoder.safetensors
# Note: stage_c_lite_bf16 is the 1B model, while stage_c_bf16 is the 3.6B model.
generator_checkpoint_path: models/stage_c_lite_bf16.safetensors

# The precision of the model.
# Preferred: bfloat16
# Usable: bfloat16, float32
dtype: bfloat16

# The learning rate.
lr: 5.0e-6
text_lr: 5.0e-6

# Loss floor
# This adds an additional amount of loss per each backwards pass.
# Avoid touching this.
loss_floor: 0

# The batch size.
batch_size: 4

# The trained image size. Must be a multiple of 32.
# 1B was trained at 768, 3.6B was trained at 1024
image_size: 1024

# How many epochs should be trained in total?
num_epochs: 10

# How many epochs should pass before saving the model?
save_every_n_epoch: 1

# How many forwards steps can the model make before performing a backwards pass?
grad_accum_steps: 1

# How many training steps can pass before saving a copy of the model?
save_every: 5000

# How many steps should the learning rate warm up by?
warmup_updates: 1
text_warmup_updates: 1

# Set a seed number of your choosing
seed: 69

# How many tokens should the model concatenate out to?
max_token_limit: 900

# On Linux, this will enable Pytorch specific optimisations for Ampere or later GPUs. (RTX 30x0 or A100)
# This option has no effect on Windows 10 and 11.
#use_pytorch_cross_attention: true

# Whether to use xformers over pytorch's flash attention
flash_attention: true

# Which optimiser you prefer using for Stage C/B.
# Recommended for bf16 training: AdafactorStoch
# Options: AdamW, AdamW8bit, Adafactor, AdafactorStoch
optimizer_type: AdafactorStoch
# The optimizer for the Text Encoder Training (Useless when not training CLIP Text):
text_optimizer_type: AdamW8bit

# All three options must be present to use EMA weights
# ema_start_iters: 5000
# ema_iters: 100
# ema_beta: 0.9

# Where files are located - to repeat a folder, add it again.
local_dataset_path: [F:\novelai]
# local_dataset_path: [F:\novelai, F:\Waifusion, F:\Fluffvision\images]

# Whether to reject images exceeding 1:x.yz ratio (Images will be tested as if they're portrait oriented - data will not be modified)
reject_aspects: 3.75

# The percentage of batches that must be duplicated for dropout purposes
# 0.1 = 10% (Default), 1 = 100%
dropout: 0.1

# Will always ignore local_dataset_path, create_latent_cache and other dataloaders if set to true
use_latent_cache: false

# Must be present when using an existing latent cache or creating one
# This value can be empty when not being used by use_latent_cache or create_latent_cache
latent_cache_location: F:\latent_cache

# Whether to create a latent cache from any of the dataloaders
create_latent_cache: false

# Whether to cache text encoder outputs (Increases latent cache size by many megabytes.)
# If you know your current folder of latent caches is fully cached, also enable this to free more
# memory. Otherwise leave this false.
cache_text_encoder: false

# Trains the text encoder.
# Using this will not allow you to cache the text encoder during latent caching.
# You have been warned.
# Training 1B under 12GB cannot be done with text encoder training.

# If you plan on training 3.6B and want to save time and memory,
# do text encoder training on the 1B model so 3.6B takes less time to
# converge on the updated text encoder weights.
train_text_encoder: false